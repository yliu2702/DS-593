{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aca90db",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting otter-grader\n",
      "  Downloading otter_grader-5.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: dill in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (0.3.7)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (3.1.3)\n",
      "Requirement already satisfied: nbformat in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (5.9.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (2.1.4)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (6.0.1)\n",
      "Collecting python-on-whales (from otter-grader)\n",
      "  Downloading python_on_whales-0.73.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (2.31.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (1.16.0)\n",
      "Collecting jupytext (from otter-grader)\n",
      "  Downloading jupytext-1.16.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (8.1.7)\n",
      "Collecting fica>=0.3.1 (from otter-grader)\n",
      "  Downloading fica-0.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: ipython in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (8.15.0)\n",
      "Requirement already satisfied: astunparse in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (1.6.3)\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (8.0.4)\n",
      "Collecting ipylab (from otter-grader)\n",
      "  Downloading ipylab-1.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (68.2.2)\n",
      "Requirement already satisfied: nbconvert in /opt/anaconda3/lib/python3.9/site-packages (from otter-grader) (7.10.0)\n",
      "Collecting docutils (from fica>=0.3.1->otter-grader)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinx (from fica>=0.3.1->otter-grader)\n",
      "  Downloading sphinx-7.4.7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.9/site-packages (from astunparse->otter-grader) (0.41.2)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /opt/anaconda3/lib/python3.9/site-packages (from astunparse->otter-grader) (1.16.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->otter-grader) (6.9.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->otter-grader) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->otter-grader) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/anaconda3/lib/python3.9/site-packages (from ipywidgets->otter-grader) (3.0.9)\n",
      "Requirement already satisfied: backcall in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (4.8.0)\n",
      "Requirement already satisfied: appnope in /opt/anaconda3/lib/python3.9/site-packages (from ipython->otter-grader) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->otter-grader) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=1.0 in /opt/anaconda3/lib/python3.9/site-packages (from jupytext->otter-grader) (3.0.0)\n",
      "Collecting mdit-py-plugins (from jupytext->otter-grader)\n",
      "  Downloading mdit_py_plugins-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.9/site-packages (from jupytext->otter-grader) (23.1)\n",
      "Requirement already satisfied: tomli in /opt/anaconda3/lib/python3.9/site-packages (from jupytext->otter-grader) (2.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (7.0.1)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (5.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (0.2.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/anaconda3/lib/python3.9/site-packages (from nbconvert->otter-grader) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /opt/anaconda3/lib/python3.9/site-packages (from nbformat->otter-grader) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/anaconda3/lib/python3.9/site-packages (from nbformat->otter-grader) (4.19.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->otter-grader) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->otter-grader) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->otter-grader) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas->otter-grader) (2023.3)\n",
      "Collecting pydantic!=2.0.*,<3,>=2 (from python-on-whales->otter-grader)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from python-on-whales->otter-grader) (4.65.0)\n",
      "Collecting typer>=0.4.1 (from python-on-whales->otter-grader)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests->otter-grader) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->otter-grader) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->otter-grader) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->otter-grader) (2023.11.17)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.9/site-packages (from bleach!=5.0.0->nbconvert->otter-grader) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6->nbconvert->otter-grader) (3.17.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (7.4.9)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.3.3)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (1.5.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython->otter-grader) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->otter-grader) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->otter-grader) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (3.10.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=1.0->jupytext->otter-grader) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->otter-grader) (0.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic!=2.0.*,<3,>=2->python-on-whales->otter-grader) (0.6.0)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic!=2.0.*,<3,>=2->python-on-whales->otter-grader)\n",
      "  Downloading pydantic_core-2.23.4-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from typer>=0.4.1->python-on-whales->otter-grader) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.9/site-packages (from typer>=0.4.1->python-on-whales->otter-grader) (13.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->otter-grader) (2.5)\n",
      "Collecting sphinxcontrib-applehelp (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sphinxcontrib-qthelp (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython->otter-grader)\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting babel>=2.13 (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting alabaster~=0.7.14 (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx->fica>=0.3.1->otter-grader)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.9/site-packages (from stack-data->ipython->otter-grader) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.9/site-packages (from stack-data->ipython->otter-grader) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.9/site-packages (from stack-data->ipython->otter-grader) (0.2.2)\n",
      "Requirement already satisfied: entrypoints in /opt/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->otter-grader) (0.4)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /opt/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->otter-grader) (23.2.0)\n",
      "Downloading otter_grader-5.6.0-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fica-0.4.1-py3-none-any.whl (13 kB)\n",
      "Downloading ipylab-1.0.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupytext-1.16.4-py3-none-any.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_on_whales-0.73.0-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdit_py_plugins-0.4.2-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinx-7.4.7-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
      "Downloading babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.3/119.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: snowballstemmer, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, pygments, pydantic-core, imagesize, docutils, babel, alabaster, sphinx, pydantic, mdit-py-plugins, typer, fica, python-on-whales, jupytext, ipylab, otter-grader\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.15.1\n",
      "    Uninstalling Pygments-2.15.1:\n",
      "      Successfully uninstalled Pygments-2.15.1\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: babel\n",
      "    Found existing installation: Babel 2.11.0\n",
      "    Uninstalling Babel-2.11.0:\n",
      "      Successfully uninstalled Babel-2.11.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.12\n",
      "    Uninstalling pydantic-1.10.12:\n",
      "      Successfully uninstalled pydantic-1.10.12\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.4.0\n",
      "    Uninstalling typer-0.4.0:\n",
      "      Successfully uninstalled typer-0.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\n",
      "langchain 0.0.179 requires pydantic<2,>=1, but you have pydantic 2.9.2 which is incompatible.\n",
      "spacy 3.2.1 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 2.9.2 which is incompatible.\n",
      "spacy 3.2.1 requires typer<0.5.0,>=0.3.0, but you have typer 0.12.5 which is incompatible.\n",
      "thinc 8.0.13 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed alabaster-0.7.16 babel-2.16.0 docutils-0.21.2 fica-0.4.1 imagesize-1.4.1 ipylab-1.0.0 jupytext-1.16.4 mdit-py-plugins-0.4.2 otter-grader-5.6.0 pydantic-2.5.3 pydantic-core-2.23.4 pygments-2.17.2 python-on-whales-0.73.0 snowballstemmer-2.2.0 sphinx-7.4.7 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 typer-0.12.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m22.0.4\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Otter\n",
    "!pip install otter-grader\n",
    "import otter\n",
    "grader = otter.Notebook(\"scraping_assignment.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f946db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045a56f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 1. Get the full names of 5 star rating books in the first page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b4960",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Check this site [https://books.toscrape.com/catalogue/page-1.html](https://books.toscrape.com/catalogue/page-1.html).\n",
    "\n",
    "This is just the first page, and there are exactly 50 pages like this, you can just change `page-1` to `page-2` and so on. \n",
    "\n",
    "For this first question, Just get the names of books from `page-1` which got 5 star rating, as a list.  Names should be full. Find `img` tag for each book by inspecting the corresponding part in page, and check for full name of the book in the `alt` attribute of `img` tag. The variable name should be `fivestar_books`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e36c8e11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sapiens: A Brief History of Humankind', 'Set Me Free', \"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\", 'Rip it Up and Start Again']\n"
     ]
    }
   ],
   "source": [
    "def get_5star_books(url):\n",
    "    # scrape the webpage, and create a BeautifulSoup object\n",
    "    # find star rating of the book -> filter out 5 star books\n",
    "    # find img tags -> find alt attribute (title of the book)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "    fivestar_books = []\n",
    "    for book in books:\n",
    "        star_rating = book.find('p', class_='star-rating Five')\n",
    "        if star_rating:\n",
    "            title = book.find('div', class_ = 'image_container').find('img')['alt']\n",
    "            fivestar_books.append(title)\n",
    "\n",
    "    return fivestar_books\n",
    "\n",
    "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
    "fivestar_books = get_5star_books(url)\n",
    "\n",
    "print(fivestar_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6951b66c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed! 🎉</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edb6eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2. Combined prices of All Saga books available in the chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e41e1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now Iterate over all 50 pages available to find books from Saga series. There are Saga 1,2,3,5,6 volumes available in the site. These you can get my matching pattern from the titles. Get the total prices of those five books. Return the float value (round nearest to two decimal points) and Don't change the currency. The variable name should be `total_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25bca842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.86\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def calculate_price_saga(pattern):\n",
    "    # scrape the webpage, and create a BeautifulSoup object\n",
    "    # find all the books with the pattern in the title\n",
    "    # find the price of the book -> sum the prices\n",
    "    Sage_price = {}\n",
    "    total_price = 0\n",
    "    for page in range(1,51):\n",
    "        url = f'https://books.toscrape.com/catalogue/page-{page}.html'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        books = soup.find_all('article', class_='product_pod')\n",
    "        for book in books:\n",
    "            sage_volume = book.find(\"div\", class_=\"image_container\").find(\"img\")[\"alt\"]\n",
    "            if pattern in sage_volume:\n",
    "                price = book.find(\"div\", class_=\"product_price\").find(\"p\", class_=\"price_color\").text\n",
    "                price_num = float(re.sub(r'[^\\d.]', '', price))\n",
    "                Sage_price[sage_volume] = price\n",
    "                total_price += price_num\n",
    "    # result = '£'+str(round(total_price, 2))\n",
    "    return Sage_price, round(total_price, 2)\n",
    "\n",
    "pattern = 'Saga, Volume'\n",
    "Sage_price_dict, total_price = calculate_price_saga(pattern)\n",
    "print(total_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e302fb2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! 🙌</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1bfce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 3. Convert the above price to Dollars using given API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5cd71",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Convert `total_price` to dollars using the given API. Pass the `total_price` to `pounds` as attribute in `params` for `requests.get`. Return float value, rounding to nearest two decimal points. The variable name should be `price_in_dollars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ef37793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.08\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "api_url = 'https://currency-converter-spark-infra-105732.apps.shift.nerc.mghpcc.org/convert'\n",
    "def convert_to_dollars(price):\n",
    "    response = requests.get(api_url, params={'pounds': price, 'convert_to': 'dollars'})\n",
    "    # response [Response] -> response.content [bytes] -> response.content.decode('utf-8') [str] -> json.loads() [dict]\n",
    "    response_json = json.loads(response.content.decode('utf-8'))\n",
    "    price_in_dollars = response_json['dollars']\n",
    "    return round(price_in_dollars,2)\n",
    "price_in_dollars = convert_to_dollars(total_price)\n",
    "print(price_in_dollars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b10722cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed! 🎉</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ae9a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## NOTE: Q 4, 5, & 6\n",
    "Check this site https://www.boxofficemojo.com/chart/top_lifetime_gross/ . For questions 4, 5, & 6, focus on the domestic only. Don't change to international.\n",
    "\n",
    "There are multiple pages, and you can use `params` argument in the `requests.get` to move across the pages, as discusssed in the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeae600",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 4. List the movies in top 200 that released in or after 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b839b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "For this question, just focus on the first page, which has top 200 (you dont have to pass any headers or params) and get the movie names released in or after 2020, as a list. The variable name should be `movies_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "08d6d86f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spider-Man: No Way Home', 'Top Gun: Maverick', 'Avatar: The Way of Water', 'Inside Out 2', 'Barbie', 'Deadpool & Wolverine', 'The Super Mario Bros. Movie', 'Black Panther: Wakanda Forever', 'Doctor Strange in the Multiverse of Madness', 'Spider-Man: Across the Spider-Verse', 'Jurassic World Dominion', 'Minions: The Rise of Gru', 'The Batman', 'Despicable Me 4', 'Guardians of the Galaxy Vol. 3', 'Thor: Love and Thunder', 'Oppenheimer', 'The Little Mermaid', 'Dune: Part Two', 'Twisters', 'Shang-Chi and the Legend of the Ten Rings', 'Wonka']\n"
     ]
    }
   ],
   "source": [
    "def get_movies(url, req_year):\n",
    "    # scrape the webpage, and create a BeautifulSoup object\n",
    "    # find all the movies released in or after 2020 in the table\n",
    "    # find the title of the movie -> save the title in a list\n",
    "    movies_list = []\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        year = row.find('td', class_ = 'a-text-left mojo-field-type-year').text.strip()\n",
    "        if int(year) >= req_year:\n",
    "            title = row.find('a').text\n",
    "            movies_list.append(title)\n",
    "    return movies_list\n",
    "\n",
    "url = 'https://www.boxofficemojo.com/chart/top_lifetime_gross/'\n",
    "year = 2020\n",
    "movies_list = get_movies(url, year)\n",
    "\n",
    "print(movies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b28d9f8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed! 🙌</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b65f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 5. List the Terminator movies present in the chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef306bd6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now Iterate over all 1000 movies available to find Terminator movies in the chart. These you can get my matching pattern from the titles. Get the list of Terminator movies present, as a list. The variable name should be `terminator_movies_list` [ Hint: Just movies which has the word `terminator` present in the title (case insensitive).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2777c1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Terminator 2: Judgment Day', 'Terminator 3: Rise of the Machines', 'Terminator Salvation', 'Terminator Genisys']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def get_movies_pattern(pattern):\n",
    "    terminator_movies_list = []\n",
    "    for page_num in range(1, 6):\n",
    "        page_url = f'{url}?offset={(page_num-1)*200}'\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the table containing the movie data\n",
    "        table = soup.find('table')\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            movie_title = row.find('a').text.strip()\n",
    "            if pattern in movie_title.lower():\n",
    "                terminator_movies_list.append(movie_title)\n",
    "    return terminator_movies_list\n",
    "\n",
    "pattern = 'terminator'\n",
    "\n",
    "terminator_movies_list = get_movies_pattern(pattern)\n",
    "\n",
    "print(terminator_movies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "854af097",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed! ✨</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dda726",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 6 \n",
    "\n",
    "Calculate the combined gross of all 4 Avengers Movies (Domestic Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "261a9588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2619552260\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def calculate_netgross_pattern(pattern):\n",
    "    avengers_gross = 0\n",
    "    avengers_patterns = re.compile(r'avengers', re.IGNORECASE)\n",
    "\n",
    "    for page_num in range(1, 6):\n",
    "        page_url = f'{url}?offset={(page_num - 1) * 200}'\n",
    "        \n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table')\n",
    "        \n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            movie_name = row.find('a').text.strip()\n",
    "            \n",
    "            if avengers_patterns.search(movie_name):\n",
    "                domestic_gross = row.find('td', class_='a-text-right mojo-field-type-money').text.strip()\n",
    "                domestic_gross_value = float(re.sub(r'[^\\d.]', '', domestic_gross))\n",
    "                avengers_gross += domestic_gross_value\n",
    "\n",
    "    return int(avengers_gross)\n",
    "\n",
    "pattern = 'avengers'\n",
    "\n",
    "avengers_gross = calculate_netgross_pattern(pattern)\n",
    "\n",
    "print(avengers_gross)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ef18957",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6</pre></strong> passed! ✨</p>"
      ],
      "text/plain": [
       "q6 results: All test cases passed!"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a79f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 7. \n",
    "\n",
    "Get the companies belonging to \"Oil and gas\" industry out of the set of 50 largest companies on this [wikipedia page](https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31f8869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Saudi Aramco', 'China Petrochemical Corporation', 'China National Petroleum Corporation', 'ExxonMobil', 'Shell', 'TotalEnergies', 'BP', 'Chevron']\n"
     ]
    }
   ],
   "source": [
    "def get_companies(url, industry):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    companies = []\n",
    "\n",
    "    table = soup.find('table', class_='wikitable sortable sticky-header-multi sort-under')\n",
    "    \n",
    "    for row in table.find_all('tr')[2:]:\n",
    "        columns = row.find_all('td')\n",
    "        company_name = columns[0].text.strip()\n",
    "        industry_name = columns[1].text.strip()\n",
    "        if industry in industry_name:\n",
    "            companies.append(company_name)\n",
    "\n",
    "    return companies\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue'\n",
    "industry = 'Oil and gas'\n",
    "\n",
    "companies_list = get_companies(url, industry)\n",
    "print(companies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "62fa784d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q7</pre></strong> passed! ✨</p>"
      ],
      "text/plain": [
       "q7 results: All test cases passed!"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q7\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(fivestar_books) == list\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(total_price) == float\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(price_in_dollars) == float\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(movies_list) == list\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(terminator_movies_list) == list\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(avengers_gross) == int\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> type(companies_list) == list\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
